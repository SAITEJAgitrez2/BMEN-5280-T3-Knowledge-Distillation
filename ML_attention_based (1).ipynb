{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vrc-bk-Dmhn",
        "outputId": "190e47ce-c131-4d0a-ccfc-9d41a61ec155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 51.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Training Teacher Model...\n",
            "Epoch 1/20, Teacher Train Acc: 0.3855, Val Acc: 0.4825, Test Acc: 0.4796\n",
            "Epoch 2/20, Teacher Train Acc: 0.5216, Val Acc: 0.5674, Test Acc: 0.5657\n",
            "Epoch 3/20, Teacher Train Acc: 0.5946, Val Acc: 0.6221, Test Acc: 0.6250\n",
            "Epoch 4/20, Teacher Train Acc: 0.6459, Val Acc: 0.6578, Test Acc: 0.6510\n",
            "Epoch 5/20, Teacher Train Acc: 0.6746, Val Acc: 0.6877, Test Acc: 0.6832\n",
            "Epoch 6/20, Teacher Train Acc: 0.7035, Val Acc: 0.7018, Test Acc: 0.7009\n",
            "Epoch 7/20, Teacher Train Acc: 0.7217, Val Acc: 0.7087, Test Acc: 0.7073\n",
            "Epoch 8/20, Teacher Train Acc: 0.7332, Val Acc: 0.7131, Test Acc: 0.7145\n",
            "Epoch 9/20, Teacher Train Acc: 0.7456, Val Acc: 0.7333, Test Acc: 0.7179\n",
            "Epoch 10/20, Teacher Train Acc: 0.7582, Val Acc: 0.7408, Test Acc: 0.7361\n",
            "Epoch 11/20, Teacher Train Acc: 0.7662, Val Acc: 0.7436, Test Acc: 0.7332\n",
            "Epoch 12/20, Teacher Train Acc: 0.7754, Val Acc: 0.7555, Test Acc: 0.7455\n",
            "Epoch 13/20, Teacher Train Acc: 0.7821, Val Acc: 0.7612, Test Acc: 0.7536\n",
            "Epoch 14/20, Teacher Train Acc: 0.7885, Val Acc: 0.7599, Test Acc: 0.7607\n",
            "Epoch 15/20, Teacher Train Acc: 0.7982, Val Acc: 0.7705, Test Acc: 0.7573\n",
            "Epoch 16/20, Teacher Train Acc: 0.7971, Val Acc: 0.7737, Test Acc: 0.7611\n",
            "Epoch 17/20, Teacher Train Acc: 0.8095, Val Acc: 0.7672, Test Acc: 0.7593\n",
            "Epoch 18/20, Teacher Train Acc: 0.8113, Val Acc: 0.7771, Test Acc: 0.7650\n",
            "Epoch 19/20, Teacher Train Acc: 0.8171, Val Acc: 0.7737, Test Acc: 0.7617\n",
            "Epoch 20/20, Teacher Train Acc: 0.8229, Val Acc: 0.7763, Test Acc: 0.7686\n",
            "\n",
            "Training Student Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-3ac2edb5de72>:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher.load_state_dict(torch.load('teacher_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Student Train Acc: 0.4978, Val Acc: 0.4993, Test Acc: 0.5003\n",
            "Epoch 2/20, Student Train Acc: 0.5913, Val Acc: 0.5808, Test Acc: 0.5865\n",
            "Epoch 3/20, Student Train Acc: 0.6265, Val Acc: 0.6213, Test Acc: 0.6178\n",
            "Epoch 4/20, Student Train Acc: 0.6532, Val Acc: 0.6477, Test Acc: 0.6434\n",
            "Epoch 5/20, Student Train Acc: 0.6710, Val Acc: 0.6610, Test Acc: 0.6590\n",
            "Epoch 6/20, Student Train Acc: 0.6834, Val Acc: 0.6772, Test Acc: 0.6731\n",
            "Epoch 7/20, Student Train Acc: 0.6994, Val Acc: 0.6851, Test Acc: 0.6870\n",
            "Epoch 8/20, Student Train Acc: 0.7097, Val Acc: 0.6994, Test Acc: 0.6928\n",
            "Epoch 9/20, Student Train Acc: 0.7136, Val Acc: 0.7014, Test Acc: 0.6927\n",
            "Epoch 10/20, Student Train Acc: 0.7208, Val Acc: 0.7009, Test Acc: 0.7004\n",
            "Epoch 11/20, Student Train Acc: 0.7310, Val Acc: 0.7140, Test Acc: 0.7037\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128  # Updated batch size\n",
        "learning_rate = 0.001  # Increased learning rate\n",
        "epochs = 20  # Reduced epochs\n",
        "lambda_attention = 0.5  # Weight for attention loss in distillation\n",
        "\n",
        "# Data loading and preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "full_train_data = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "train_size = int(0.8 * len(full_train_data))\n",
        "val_size = len(full_train_data) - train_size\n",
        "train_data, val_data = random_split(full_train_data, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Teacher Model\n",
        "class TeacherNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self._to_linear = None\n",
        "        self.convs = nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU(), self.conv3, nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self._get_flattened_size()\n",
        "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def _get_flattened_size(self):\n",
        "        with torch.no_grad():\n",
        "            x = torch.rand(1, 3, 32, 32)\n",
        "            x = self.convs(x)\n",
        "            self._to_linear = x.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Student Model\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self._to_linear = None\n",
        "        self.convs = nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self._get_flattened_size()\n",
        "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def _get_flattened_size(self):\n",
        "        with torch.no_grad():\n",
        "            x = torch.rand(1, 3, 32, 32)\n",
        "            x = self.convs(x)\n",
        "            self._to_linear = x.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Attention Loss Function\n",
        "def attention_loss(teacher_feature, student_feature):\n",
        "    teacher_attention = F.normalize(teacher_feature.pow(2).mean(1).view(teacher_feature.size(0), -1))\n",
        "    student_attention = F.normalize(student_feature.pow(2).mean(1).view(student_feature.size(0), -1))\n",
        "    return F.mse_loss(student_attention, teacher_attention)\n",
        "\n",
        "# Train function\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "    return total_loss / len(loader), correct / len(loader.dataset)\n",
        "\n",
        "# Evaluate function\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "    return total_loss / len(loader), correct / len(loader.dataset)\n",
        "\n",
        "# Count Parameters\n",
        "def calculate_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# CUDA setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize models\n",
        "teacher = TeacherNet().to(device)\n",
        "student = StudentNet().to(device)\n",
        "teacher_optimizer = optim.Adam(teacher.parameters(), lr=learning_rate)\n",
        "student_optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track Metrics\n",
        "teacher_val_acc_list, teacher_test_acc_list, teacher_train_acc_list = [], [], []\n",
        "student_val_acc_list, student_test_acc_list, student_train_acc_list = [], [], []\n",
        "\n",
        "# Train Teacher Model\n",
        "print(\"Training Teacher Model...\")\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = train(teacher, train_loader, teacher_optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(teacher, val_loader, criterion)\n",
        "    test_loss, test_acc = evaluate(teacher, test_loader, criterion)\n",
        "\n",
        "    teacher_train_acc_list.append(train_acc)\n",
        "    teacher_val_acc_list.append(val_acc)\n",
        "    teacher_test_acc_list.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Teacher Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# Save Teacher Model\n",
        "torch.save(teacher.state_dict(), 'teacher_model.pth')\n",
        "\n",
        "# Train Student Model with Attention Distillation\n",
        "print(\"\\nTraining Student Model...\")\n",
        "teacher.load_state_dict(torch.load('teacher_model.pth'))\n",
        "teacher.eval()\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    student.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            teacher_features = teacher.convs(images)\n",
        "        student_features = student.convs(images)\n",
        "        student_output = student(images)\n",
        "        classification_loss = criterion(student_output, labels)\n",
        "        att_loss = attention_loss(teacher_features, student_features)\n",
        "        loss = classification_loss + lambda_attention * att_loss\n",
        "        student_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        student_optimizer.step()\n",
        "\n",
        "    val_loss, val_acc = evaluate(student, val_loader, criterion)\n",
        "    test_loss, test_acc = evaluate(student, test_loader, criterion)\n",
        "    train_loss, train_acc = evaluate(student, train_loader, criterion)\n",
        "\n",
        "    student_train_acc_list.append(train_acc)\n",
        "    student_val_acc_list.append(val_acc)\n",
        "    student_test_acc_list.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Student Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(student.state_dict(), 'student_model.pth')\n",
        "\n",
        "# Final Model Evaluations\n",
        "teacher_params = calculate_params(teacher)\n",
        "student_params = calculate_params(student)\n",
        "compression_rate = student_params / teacher_params\n",
        "\n",
        "print(f\"\\nTeacher Model Parameters: {teacher_params}\")\n",
        "print(f\"Student Model Parameters: {student_params}\")\n",
        "print(f\"Compression Rate: {compression_rate:.4f}\")\n",
        "\n",
        "# Plot Accuracy Graphs\n",
        "def plot_accuracy(title, train_acc, val_acc, test_acc):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, epochs + 1), train_acc, label=f\"{title} Train Accuracy\")\n",
        "    plt.plot(range(1, epochs + 1), val_acc, label=f\"{title} Validation Accuracy\")\n",
        "    plt.plot(range(1, epochs + 1), test_acc, label=f\"{title} Test Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(f\"{title} Model Accuracy Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot teacher and student accuracies\n",
        "plot_accuracy(\"Teacher\", teacher_train_acc_list, teacher_val_acc_list, teacher_test_acc_list)\n",
        "plot_accuracy(\"Student\", student_train_acc_list, student_val_acc_list, student_test_acc_list)\n",
        "\n",
        "# Plot Graph Comparing Validation and Test Accuracy\n",
        "def plot_validation_test_accuracy(title, val_acc, test_acc):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, epochs + 1), val_acc, label=f\"{title} Validation Accuracy\", linestyle='dashed')\n",
        "    plt.plot(range(1, epochs + 1), test_acc, label=f\"{title} Test Accuracy\", linestyle='solid')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(f\"{title}: Validation vs Test Accuracy Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot Teacher and Student Graphs\n",
        "plot_validation_test_accuracy(\"Teacher Model\", teacher_val_acc_list, teacher_test_acc_list, )\n",
        "plot_validation_test_accuracy(\"Student Model\", student_val_acc_list, student_test_acc_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Parameter and Accuracy Reduction\n",
        "teacher_accuracy = max(teacher_test_acc_list) * 100  # Convert to percentage\n",
        "student_accuracy = max(student_test_acc_list) * 100  # Convert to percentage\n",
        "param_reduction = ((teacher_params - student_params) / teacher_params) * 100\n",
        "accuracy_reduction = teacher_accuracy - student_accuracy\n",
        "\n",
        "# Plot Parameter and Accuracy Reduction\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(['Parameter Reduction (%)', 'Accuracy Reduction (%)'], [param_reduction, accuracy_reduction], color=['skyblue', 'salmon'])\n",
        "plt.ylabel('Percentage Reduction')\n",
        "plt.title('Reduction in Parameters and Accuracy from Teacher to Student Model')\n",
        "plt.show()\n",
        "\n",
        "# Normalize parameters to millions for better visualization\n",
        "teacher_params_normalized = teacher_params / 1e6\n",
        "student_params_normalized = student_params / 1e6\n",
        "\n",
        "# Prepare categories and values for comparison plot\n",
        "categories = ['Teacher Parameters (M)', 'Teacher Accuracy (%)', 'Student Parameters (M)', 'Student Accuracy (%)']\n",
        "values = [teacher_params_normalized, teacher_accuracy, student_params_normalized, student_accuracy]\n",
        "\n",
        "# Plot Comparison of Parameters and Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(categories, values, color=['skyblue', 'blue', 'salmon', 'red'])\n",
        "plt.ylabel('Values (Parameters in Millions, Accuracy in %)')\n",
        "plt.title('Comparison of Parameters and Accuracy between Teacher and Student Models (Normalized)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ws1ThNo3epxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}