import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Hyperparameters
batch_size, learning_rate, epochs = 64, 0.0005, 30
lambda_attention = 1.0  # Initial weight for attention loss
final_lambda_attention = 0.1  # Minimum weight
lambda_decay = 0.95  # Decay rate

# Data Transformations
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 Dataset
full_train_data = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
train_size = int(0.8 * len(full_train_data))
train_data, val_data = random_split(full_train_data, [train_size, len(full_train_data) - train_size])

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transform, download=True),
                         batch_size=batch_size, shuffle=False)

# Define Teacher Model
class TeacherNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.convs = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)
        )
        self._to_linear = self._get_flattened_size()
        self.fc1 = nn.Linear(self._to_linear, 512)
        self.fc2 = nn.Linear(512, 10)

    def _get_flattened_size(self):
        with torch.no_grad():
            x = torch.rand(1, 3, 32, 32)
            x = self.convs(x)
        return x.numel()

    def forward(self, x, return_features=False):
        x = self.convs(x)
        features = x.view(x.size(0), -1)
        x = F.relu(self.fc1(features))
        out = self.fc2(x)
        return (out, features) if return_features else out

# Define Student Model
class StudentNet(nn.Module):
    def __init__(self, conv1_out_channels, conv2_out_channels, fc1_out_features):
        super().__init__()
        self.convs = nn.Sequential(
            nn.Conv2d(3, conv1_out_channels, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv2d(conv1_out_channels, conv2_out_channels, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)
        )
        self._to_linear = self._get_flattened_size()
        self.fc1 = nn.Linear(self._to_linear, fc1_out_features)
        self.fc2 = nn.Linear(fc1_out_features, 10)

    def _get_flattened_size(self):
        with torch.no_grad():
            x = torch.rand(1, 3, 32, 32)
            x = self.convs(x)
        return x.numel()

    def forward(self, x, return_features=False):
        x = self.convs(x)
        features = x.view(x.size(0), -1)
        x = F.relu(self.fc1(features))
        out = self.fc2(x)
        return (out, features) if return_features else out

# Define Attention Transfer Loss Function
def attention_transfer_loss(teacher_feature, student_feature):
    """
    Compute the attention loss between teacher and student feature maps.
    Ensures correct feature alignment and normalization.
    """
    def compute_attention_map(feature):
        # Compute channel-wise sum of squares
        attention_map = feature.pow(2).mean(1)  # Mean over channels
        return F.normalize(attention_map.view(feature.size(0), -1), p=2, dim=1)  # Normalize per sample

    teacher_attention = compute_attention_map(teacher_feature)
    student_attention = compute_attention_map(student_feature)

    return F.mse_loss(student_attention, teacher_attention)  # MSE between attention maps

# Train Function with Attention-Based KD
def train_attention_kd(model, teacher_model, loader, optimizer, criterion, lambda_attention):
    model.train()
    total_loss, correct = 0, 0
    for images, labels in loader:
        images, labels = images.cuda(), labels.cuda()
        optimizer.zero_grad()
        
        student_outputs, student_features = model(images, return_features=True)
        with torch.no_grad():
            teacher_outputs, teacher_features = teacher_model(images, return_features=True)

        classification_loss = criterion(student_outputs, labels)
        att_loss = attention_transfer_loss(teacher_features, student_features)
        loss = classification_loss + lambda_attention * att_loss

        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        correct += (student_outputs.argmax(dim=1) == labels).sum().item()

    return total_loss / len(loader), correct / len(loader.dataset)

# Validation Function
def validate(model, loader, criterion):
    model.eval()
    total_loss, correct = 0, 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.cuda(), labels.cuda()
            outputs = model(images)
            total_loss += criterion(outputs, labels).item()
            correct += (outputs.argmax(dim=1) == labels).sum().item()
    return total_loss / len(loader), correct / len(loader.dataset)

# Train Teacher Model
teacher = TeacherNet().cuda()
teacher_optimizer = optim.Adam(teacher.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()

print("Training Teacher Model...")
for epoch in range(epochs):
    train_loss, train_acc = train_attention_kd(teacher, teacher, train_loader, teacher_optimizer, criterion, 0)  # No KD for teacher
    val_loss, val_acc = validate(teacher, val_loader, criterion)
    print(f"Epoch {epoch+1}/{epochs}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

torch.save(teacher.state_dict(), "best_teacher_model.pth")

# Train Student Models
student_sizes = [(16, 32, 64), (32, 64, 128), (48, 96, 256)]
for conv1_out, conv2_out, fc1_out in student_sizes:
    print(f"\nTraining Student Model (Size: {conv1_out}x{conv2_out}x{fc1_out})")
    student = StudentNet(conv1_out, conv2_out, fc1_out).cuda()
    student_optimizer = optim.Adam(student.parameters(), lr=learning_rate)
    lambda_att = lambda_attention

    for epoch in range(epochs):
        train_loss, train_acc = train_attention_kd(student, teacher, train_loader, student_optimizer, criterion, lambda_att)
        val_loss, val_acc = validate(student, val_loader, criterion)
        print(f"Epoch {epoch+1}/{epochs}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")
        lambda_att = max(final_lambda_attention, lambda_att * lambda_decay)  # Decay lambda_att

    torch.save(student.state_dict(), f"best_attention_student_model_{conv1_out}_{conv2_out}_{fc1_out}.pth")
